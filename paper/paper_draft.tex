\documentclass{article}

% Preamble
\usepackage{url}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{pbox}

\newcommand{\xmark}{\ding{55}}%

% Document
\begin{document}

\begin{abstract}
The study and adoption of deep learning methods has led to significant progress in different application domains. As deep learning continues to show promise and its utilization matures, so does the infrastructure and software needed to support it. Various frameworks have been developed in recent years to facilitate both implementation and training of deep learning networks. As deep learning has also evolved to becoming distributed, there's a growing need for frameworks that can support execution across many machine. While deep learning frameworks restricted to running on a single machine have been studied and compared, frameworks which support deep learning distributed across multiple machines are relatively less known and well-studied. This paper seeks to bridge that gap by surveying, summarizing, and comparing frameworks which currently support distributed execution, including Tensorflow, Deeplearning4j, MXNet, H2O, and CaffeOnSpark. 
\end{abstract}

\section{Introduction}
Deep learning has been quite successful in improving predictive power in domains such as computer vision and natural language processing. As accuracies continue to  increase, so do the complexity of network architectures and the size of the parameter space. Google's network for unsupervised learning of image features reached a billion parameters \cite{donahue2014decaf}, and was increased to 11 billion parmeters in a separate experiment at Stanford \cite{schmidhuber2015deep}. In the NLP space, Digital Reasoning Systems trained a 160 billion parameter network \cite{trask2015modeling} fairly recently. Handling problems of this size involves scaling up to thousands of cores across many machines, which Google first demonstrated on its distributed DistBelief framework \cite{dean2012large}.

The goal of this paper is to survey the landscape of deep learning frameworks with distributed execution, and compare them across a consistent set of characteristics. These characteristics include release date, core language, user-facing API, computation model, communication model, data parallelism, model parallelism, programming paradigm, fault tolerance, and visualization. This choice of criteria is explained in detail in Section 2. Tensorflow, Deeplearning4j, MXNet, H2O, and CaffeOnSpark were chosen by a combination of factors, including their being open-source, level of documentation, maturity as a product, and adoption by the community. Frameworks currently without distributed support won't be the focus of this discussion, but well-known ones such as Theano, Torch, Caffe (without Spark) have been studied in the past \cite{DBLP:journals/corr/BahrampourRSS15}. 

The rest of the paper is organized as follows: Section 2 explains the set of comparison criteria and summarizes the distributed frameworks according that criteria. Section 3 discusses each distributed framework in detail. Section 4 outlines future directions of work. Section 5 concludes the paper. 

\section{Framework Comparison}
\begin{tabularx}{1.35\textwidth}{ |X|X|X|X|X|X| }
  \hline
  Platform & Tensorflow & Deeplearning4j & MXNet & H2O & CaffeOnSpark \\
  \hline
  Release Date & 2016 (distributed) & 2015 & 2015 & 2014 (deep learning) & 2016 \\
   \hline
  Core Language & C++  & Java  & C++ & Java & \pbox{20cm}{C++ (Caffe) \\ Scala (Spark)}  \\
  \hline
  API & C++, Python & Java, Scala & C++, Python, R, Scala, Matlab, Javascript, Go, Julia & Java, R, Python, Scala, Javascript, web-UI & Python, Matlab, Scala \\
  \hline
  Computation Model & Sync or async & Sync & Sync or async & Async & Sync \\
  \hline
  Communication Model & Parameter server & Iterative MapReduce & Parameter server & Distributed fork-join & MPI Allreduce \\
  \hline
  Data Parallelism & \checkmark & \checkmark & \checkmark& \checkmark & \checkmark \\
  \hline
  Model Parallelism & \checkmark & \xmark & \checkmark & \xmark & \xmark \\
  \hline
  Programming Paradigm & Imperative & Declarative & Both & Declarative & Declarative \\
  \hline
  Fault Tolerance & Checkpoint-and-recovery & Checkpoint-and-resume & Checkpoint-and-resume & N/A & N/A \\
  \hline
  Visualization & Graph-visualization, training monitoring & Training monitoring & None & None & Summary Statistics \\
  \hline
\end{tabularx}

The relevance of release date, core language, user-facing APIs are self-explanatory. Computation model specifies the nature of data consistency through execution, i.e. whether updates are synchronous or asynchronous. In context of optimization kernels like stochastic gradient descent (SGD), synchronous execution has better convergence guarantees by maintaining consistency or near-consistency with sequential execution. However, asynchronous SGD can exploit more parallelism and train faster, but with less guarantees of convergence speed. Note that frameworks like Tensorflow and MXNet simply leave this as a tradeoff choice for the user. 

The communication model tries to categorize the nature of distributed execution by well-known paradigms. 

Data and model parallelism are the two prevalent opportunities for parallelism in training deep learning networks at the distributed level. In data parallelism, copies of the model, or parameters, are each trained on its own subset of the training data, while updating the same global model. In model parallelism, the model itself is partitioned and trained in parallel. 

Programming paradigm falls into the categories of imperative, declarative, or a mix of both. Conventionally, imperative programming specifies \textit{how} a computation is done, where as declarative programming specifies \textit{what} needs to be done. There is plenty of gray area, but the distinction is made in this paper based on whether the API exposes the user to computation details that require some understanding of the inner math of neural networks (imperative), or whether the abstraction is yet higher (declarative). 

Fault tolerance is included for two reasons. Distributed execution tends to be more failure prone, especially at scale. Furthermore, any failures (not necessarily limited to distributed execution) that interrupt training part-way can be very costly, if all the progress made on the model is simply lost. 

Finally, UI/Visualization is a feature supported to very different degrees across the frameworks studied. The ability to monitor the progress of training and the internal state of networks over time could be useful for debugging or hyperparameter tuning, and could be an interesting direction. Tensorflow and Deeplearning4j both support this kind of visualization. 

The scope of deep learning functionality was a possible point of discussion. However, initial surveying suggested that at this point most deep learning libraries provide all the standard network architectures, training and optimization techniques. More differences are likely to be found in programmability or performance. 

\section{Framework Discussion}

\subsection{Tensorflow}
Tensorflow was released by Google Research as open source in November 2015, and included distributed support in 2016. The user-facing APIs are C++ and Python. Programming with Tensorflow leans more imperative. While plenty of abstraction power is expressed in its library, the user will probably also be working with computational primitive wrappers such as matrix operations, element-wise math operators, and looping control. In other words, the user is exposed to some of the internal workings of deep learning networks. Tensorflow treats networks as a directed graph of nodes encapsulating dataflow computation and required dependencies \cite{DBLP:journals/corr/AbadiABBCCCDDDG16}. Each node, or computation, gets mapped to devices (CPUs or GPUs) according to some cost function. This partitions the overall graph into subgraphs, one per device. Cross-device edges are replaced to encode necessary synchronization between device pairs. Distributed execution appears to be a natural extension of this arrangement, except that TCP or Remote Direct Memory Access (RDMA) is used for inter-device communication on separate machines. This approach of mapping subgraphs onto devices also offers potential scalability, because each worker can schedule its own subgraph at runtime instead of relying on a centralized master \cite{DBLP:journals/corr/AbadiABBCCCDDDG16}. Parallelism in Tensorflow can be expressed at several levels, notably both data parallelism and model parallelism. Data parallelism can happen both across and within workers, by training separate batches of data on model replications. Model parallelism is expressed through splitting one model, or its graph, across devices. Model updates can either be synchronous or asynchronous for parameter-optimizing algorithms such as SGD. For fault tolerance, Tensorflow provides checkpointing and recovery of data designated to be persistent, while the overall computation graph is restarted. In terms of other features, TensorBoard is a tool for interactive visualization of a user's network, and also provides time series data on various aspects of the learning network's state during training. 

\subsection{Deeplearning4j}
Deeplearning4j is a Java-based deep learning library built and supported by Skymind, a machine learning intelligence company, in 2014. It is an open source product designed for adoptability in industry, where Java is very common. The framework currently interfaces with both Java and Scala, with a Python SDK in-progress. Programming is primarily declarative, involving specifying network hyperparameters and layer information. Deeplearning4j integrates with Hadoop and Spark, or Akka and AWS for processing backends. Distributed execution provides data parallelism through the Iterative MapReduce model \cite{Itera38:online}. Each worker processes its own minibatch of training data, with workers periodically "reducing" (averaging) their parameter data. Deeplearning4j hosts its own Java linear algebra library, which claims 2x or more speedup over Python's Numpy library on large matrix multiplies \cite{ND4J'34:online}. Fault tolerance is not mentioned, although Spark does have built-in fault tolerance mechanisms. 

\subsection{MXNet}
MXNet became available in 2015 and was developed in collaboration across several institutions, including CMU, University of Washington, and Microsoft. It currently interfaces with C++, Python, R, Scala, Matlab, Javascript, Go, and Julia. MXNet supports both declarative and declarative expressions; declarative in declaring computation graphs with higher-level abstractions like convolutional layers, and imperative in the ability to direct tensor computation and control flow. Data parallelism is supported by default, and it also seems possible to build with model parallelism. Distributed execution in MXNet generally follows a parameter server model, with parallelism and data consistency managed at two levels: intra-worker and inter-worker \cite{chen2015mxnet}. Devices within a single worker machine maintain synchronous consistency on its parameters. Inter-worker data consistency can either be synchronous, where gradients over all workers are aggregated before proceeding, or asynchronous, where each worker independently updates parameters. This trade-off between performance and convergence speed is left as an option to the user. The actual handling of server updates and requests is pushed down to MXNet's dependency engine, which schedules all operations and performs resource management \cite{chen2015mxnet}. Fault tolerance on MXNet involves checkpoint-and-resume, which must be user-initiated. 

\subsection{H2O}
H2O is the open-source product of H2O.ai, a company focused on machine learning solutions. H2O is unique among the other tools discussed here in that it is a complete data processing platform, with its own parallel processing engine (improving on MapReduce) with a general machine learning library. The discussion will be limited to H2O's deep learning component, available since 2014. H2O is Java-based at its core, but also offers API support for Java, R, Python, Scala, Javascript, as well as a web-UI interface \cite{candel2015deep}. Programming for deep learning appears declarative, as model-building involves specifying hyperparameters and high-level layer information. Distributed execution for deep learning follows the characteristics of H2O's processing engine, which is in-memory and can be summarized as a distributed fork-join model (targeting finer-grained parallelism) \cite{Landset2015}. Data parallelism follows the "HogWild!" \cite{recht2011hogwild} approach for parallelizing SGD. Multiple cores handle subsets of training data and update shared parameters asynchronously. Scaling up to multi-node, each node operates in parallel on a copy of the global parameters, while parameters are averaged for a global update, per training iteration \cite{candel2015deep}. There does not seem to be explicit support for model parallelism. Fault tolerance involves user-initiated checkpoint-and-resume. H2O's web tool can be used to build models and manage workflows, as well as some basic summary statistics, e.g. confusion matrix from training and validation. 

\subsection{CaffeOnSpark}
CaffeOnSpark is a Spark deep learning package released open-source in early 2016 by Yahoo's Big ML team. It serves as a distributed implementation of Caffe, a framework for convolutional deep learning released by UC Berkeley's computer vision community in 2014. The language interface for CaffeOnSpark is Scala (following Spark), while Caffe itself offers Python and Matlab API. Programming is highly declarative; creating a deep learning network involves specifying layers and hyperparameters, which are compiled down to a configuration file that Caffe then uses. During distributed runtime, Spark launches "executors," each responsible for a partition of HDFS-based training data and trains the data by running multiple Caffe threads mapped to GPUs \cite{Large52:online}. MPI is used to synchronize executor's respective the parameters' gradients in an Allreduce-like fashion, per training batch \cite{Caffe27:online}. In terms of other notable features, Caffe itself hosts a repository of pre-trained models of some popular convolutional networks such as AlexNet or GoogleNet. It also integrates support for data preprocessing, including building LMDB databases from raw data for higher-throughput, concurrent reading. It does not seem that CaffeOnSpark presently offers any fault tolerance other than what comes with Spark.

\section{Future Work}
\section{Conclusion}

\bibliography{works_cited} 
\bibliographystyle{ieeetr}

\end{document}

