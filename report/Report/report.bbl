\begin{thebibliography}{10}

\bibitem{Caffe27:online}
Caffeonspark open sourced for distributed deep... | hadoop at yahoo.
\newblock
  \url{http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep}.
\newblock (Accessed on 07/23/2016).

\bibitem{DIGIT88:online}
Digits: Deep learning gpu training system | parallel forall.
\newblock
  \url{https://devblogs.nvidia.com/parallelforall/digits-deep-learning-gpu-training-system/}.
\newblock (Accessed on 07/28/2016).

\bibitem{GP1004:online}
Gp100 pascal whitepaper.
\newblock
  \url{https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf}.
\newblock (Accessed on 07/27/2016).

\bibitem{AnInt85:online}
An introduction to computational networks and the computational network
  toolkit.
\newblock
  \url{http://www.zhihenghuang.com/publications/CNTKBook-Draft0.7-2015-03-19.pdf}.
\newblock (Accessed on 07/27/2016).

\bibitem{Itera38:online}
Iterative reduce with dl4j on hadoop and spark - deeplearning4j: Open-source,
  distributed deep learning for the jvm.
\newblock \url{http://deeplearning4j.org/iterativereduce}.
\newblock (Accessed on 07/26/2016).

\bibitem{Large52:online}
Large scale distributed deep learning on hadoop... | hadoop at yahoo.
\newblock
  \url{http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop}.
\newblock (Accessed on 07/23/2016).

\bibitem{Multi35:online}
Multiple gpus and machines · microsoft/cntk wiki.
\newblock
  \url{https://github.com/Microsoft/CNTK/wiki/Multiple-GPUs-and-machines}.
\newblock (Accessed on 07/27/2016).

\bibitem{ND4J'34:online}
Nd4j's benchmarking tool n-dimensional scientific computing for java.
\newblock \url{http://nd4j.org/benchmarking}.
\newblock (Accessed on 07/26/2016).

\bibitem{Power24:online}
Powerpoint presentation.
\newblock
  \url{http://research.microsoft.com/en-us/um/people/dongyu/CNTK-Tutorial-NIPS2015.pdf}.
\newblock (Accessed on 07/27/2016).

\bibitem{Tenso77:online}
Tensorflow meets microsoft’s cntk | the escience cloud.
\newblock
  \url{https://esciencegroup.com/2016/02/08/tensorflow-meets-microsofts-cntk/}.
\newblock (Accessed on 07/27/2016).

\bibitem{Torch91:online}
Torch | scientific computing for luajit.
\newblock \url{http://torch.ch/}.
\newblock (Accessed on 07/28/2016).

\bibitem{DBLP:journals/corr/AbadiABBCCCDDDG16}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S. Corrado,
  A.~Davis, J.~Dean, M.~Devin, S.~Ghemawat, I.~J. Goodfellow, A.~Harp,
  G.~Irving, M.~Isard, Y.~Jia, R.~J{\'{o}}zefowicz, L.~Kaiser, M.~Kudlur,
  J.~Levenberg, D.~Man{\'{e}}, R.~Monga, S.~Moore, D.~G. Murray, C.~Olah,
  M.~Schuster, J.~Shlens, B.~Steiner, I.~Sutskever, K.~Talwar, P.~A. Tucker,
  V.~Vanhoucke, V.~Vasudevan, F.~B. Vi{\'{e}}gas, O.~Vinyals, P.~Warden,
  M.~Wattenberg, M.~Wicke, Y.~Yu, and X.~Zheng.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em CoRR}, abs/1603.04467, 2016.

\bibitem{candel2015deep}
A.~Candel, V.~Parmar, E.~LeDell, and A.~Arora.
\newblock Deep learning with h2o, 2015.

\bibitem{catanzaro2013deep}
B.~Catanzaro.
\newblock Deep learning with cots hpc systems.

\bibitem{chen2015mxnet}
T.~Chen, M.~Li, Y.~Li, M.~Lin, N.~Wang, M.~Wang, T.~Xiao, B.~Xu, C.~Zhang, and
  Z.~Zhang.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock {\em arXiv preprint arXiv:1512.01274}, 2015.

\bibitem{dean2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, A.~Senior, P.~Tucker,
  K.~Yang, Q.~V. Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1223--1231, 2012.

\bibitem{donahue2014decaf}
J.~Donahue, Y.~Jia, O.~Vinyals, J.~Hoffman, N.~Zhang, E.~Tzeng, and T.~Darrell.
\newblock Decaf: A deep convolutional activation feature for generic visual
  recognition.
\newblock In {\em ICML}, pages 647--655, 2014.

\bibitem{iandola2015firecaffe}
F.~N. Iandola, K.~Ashraf, M.~W. Moskewicz, and K.~Keutzer.
\newblock Firecaffe: near-linear acceleration of deep neural network training
  on compute clusters.
\newblock {\em arXiv preprint arXiv:1511.00175}, 2015.

\bibitem{jia2014caffe}
Y.~Jia, E.~Shelhamer, J.~Donahue, S.~Karayev, J.~Long, R.~Girshick,
  S.~Guadarrama, and T.~Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In {\em Proceedings of the 22nd ACM international conference on
  Multimedia}, pages 675--678. ACM, 2014.

\bibitem{Landset2015}
S.~Landset, T.~M. Khoshgoftaar, A.~N. Richter, and T.~Hasanin.
\newblock A survey of open source tools for machine learning with big data in
  the hadoop ecosystem.
\newblock {\em Journal of Big Data}, 2(1):1--36, 2015.

\bibitem{DBLP:journals/corr/MaMT16}
H.~Ma, F.~Mao, and G.~W. Taylor.
\newblock Theano-mpi: a theano-based distributed training framework.
\newblock {\em CoRR}, abs/1605.08325, 2016.

\bibitem{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  693--701, 2011.

\bibitem{schmidhuber2015deep}
J.~Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural Networks}, 61:85--117, 2015.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1--9, 2015.

\bibitem{2016arXiv160502688short}
{Theano Development Team}.
\newblock {Theano: A {Python} framework for fast computation of mathematical
  expressions}.
\newblock {\em arXiv e-prints}, abs/1605.02688, May 2016.

\bibitem{trask2015modeling}
A.~Trask, D.~Gilmore, and M.~Russell.
\newblock Modeling order in neural word embeddings at scale.

\bibitem{yu2015computational}
D.~Yu, K.~Yao, and Y.~Zhang.
\newblock The computational network toolkit [best of the web].
\newblock {\em IEEE Signal Processing Magazine}, 32(6):123--126, 2015.

\end{thebibliography}
