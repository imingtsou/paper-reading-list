\section{Framework Comparison}
\begin{table*}
\small
\centering
\caption{Open-source Frameworks}
\resizebox{\textwidth}{!}{
    \begin{tabular}{|m{2cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|m{2cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|m{1.7cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|}
    \hline
    Platform & Tensorflow & CNTK & Deeplearning4j & MXNet & H2O & Caffe & Theano & Torch \\ \hline\hline
    Release Date & 2016 & 2016 & 2015 & 2015 & 2014 & 2014 & 2010 & 2011 (deep learning) \\
    \hline
    Core Language & C++  & C++ & Java  & C++ & Java & C++ & C++ & C \\
    \hline
    API & C++, Python & NDL & Java, Scala & C++, Python, R, Scala, Matlab, Javascript, Go, Julia & Java, R, Python, Scala, Javascript, web-UI & Python, Matlab & Python & Lua \\
    \hline
    Synchronization Model & Sync or async & Sync & Sync & Sync or async & Async & Sync & Async & Sync \\
    \hline
    Communication Model & Parameter server & MPI & Iterative MapReduce & Parameter server & Distributed fork-join & N/A & N/A & N/A \\
    \hline
    Multi-GPU & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
    \hline 
    Multi-node & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \xmark \\
    \hline
    Data Parallelism & \cmark & \cmark & \cmark & \cmark& \cmark & \cmark & \cmark & \cmark \\
    \hline
    Model Parallelism & \cmark & N/A & \xmark & \cmark & \xmark & \xmark & \cmark & \cmark \\
    \hline
    Deep Learning Models & DBN, CNN, RNN & DBN, CNN, RNN & DBN, CNN, RNN & DBN, CNN, RNN & DBN & DBN, CNN, RNN & DBN, CNN, RNN & DBN, CNN, RNN \\
    \hline
    Programming Paradigm & Imperative & Imperative & Declarative & Both & Declarative & Declarative & Imperative & Imperative \\
    \hline
    Fault Tolerance & Checkpoint-and-recovery & Checkpoint-and-resume & Checkpoint-and-resume & Checkpoint-and-resume & N/A & N/A & Checkpoint-and-resume & Checkpoint-and-resume \\
    \hline
    Visualization & Graph (interactive), training monitoring & Graph (static) & Training monitoring & None & None & Summary Statistics & Graph (static) & Plots \\
    \hline
    \end{tabular}
}
\end{table*}

The relevance of release date, core language, user-facing APIs are self-explanatory. Synchronization model specifies the nature of data consistency through execution, i.e. whether updates are synchronous or asynchronous. In context of optimization kernels like stochastic gradient descent (SGD), synchronous execution has better convergence guarantees by maintaining consistency or near-consistency with sequential execution. Asynchronous SGD can exploit more parallelism and train faster, but with less guarantees of convergence speed. Frameworks like Tensorflow and MXNet leave this tradeoff as a choice to the user. 

The communication model tries to categorize the nature of across-machine execution according to well-known paradigms.

There are three possible levels of parallelism at the hardware level: cores within a CPU/GPU device, across multiple devices (usually GPUs for deep learning), or across machines. Most lower-level library kernels (e.g. for linear algebra) are designed to use multiple cores of a device by default, so this is not a major point of comparison. At this point, all the frameworks also support parallelism across multiple GPUs. Theano and Torch do not yet support multi-machine parallelism. 

Data and model parallelism are the two prevalent opportunities for parallelism in training deep learning networks at the distributed level. In data parallelism, copies of the model, or parameters, are each trained on its own subset of the training data, while updating the same global model. In model parallelism, the model itself is partitioned and trained in parallel. 

Deep learning models can be categorized into three major types: deep-belief networks (DBNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs). CNNs and RNNs were briefly described in the introduction. DBNs are less domain-specific compared to CNNs and RNNs, and could be considered a precursor to CNNs, but are fundamental nonetheless. 

Programming paradigm falls into the categories of imperative, declarative, or a mix of both. Conventionally, imperative programming specifies \textit{how} a computation is done, where as declarative programming specifies \textit{what} needs to be done. There is plenty of gray area, but the distinction is made in this paper based on whether the API exposes the user to computation details that require some understanding of the inner math of neural networks (imperative), or whether the abstraction is yet higher (declarative). 

Fault tolerance is included for two reasons. Distributed execution tends to be more failure prone, especially at scale. Furthermore, any failures (not necessarily limited to distributed execution) that interrupt training part-way can be very costly, if all the progress made on the model is simply lost. 

Finally, UI/Visualization is a feature supported to very different degrees across the frameworks studied. The ability to monitor the progress of training and the internal state of networks over time could be useful for debugging or hyperparameter tuning, and could be an interesting direction. Tensorflow and Deeplearning4j both support this kind of visualization. 
