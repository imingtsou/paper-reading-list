\section{Introduction}
Deep learning has been quite successful in improving predictive power in domains such as computer vision and natural language processing. As accuracies continue to  increase, so do the complexity of network architectures and the size of the parameter space. Google's network for unsupervised learning of image features reached a billion parameters \cite{donahue2014decaf}, and was increased to 11 billion parmeters in a separate experiment at Stanford \cite{schmidhuber2015deep}. In the NLP space, Digital Reasoning Systems trained a 160 billion parameter network \cite{trask2015modeling} fairly recently. Handling problems of this size involves looking beyond the single machine, which Google first demonstrated through its distributed DistBelief framework \cite{dean2012large}.

The goal of this paper is to survey the landscape of deep learning frameworks with distributed execution, and compare them across a consistent set of characteristics. These characteristics include release date, core language, user-facing API, computation model, communication model, data parallelism, model parallelism, programming paradigm, fault tolerance, and visualization. This choice of criteria is explained in detail in Section 2. Tensorflow, CNTK, Deeplearning4j, MXNet, H2O, and CaffeOnSpark were chosen by a combination of factors, including their being open-source, level of documentation, maturity as a product, and adoption by the community. These frameworks are examined in detail in Section 3. Additional frameworks without built-in distributed support, such as Theano, Torch, and native Caffe, are mentioned alongside known external distributed implementations. Finally, the nuances of the relationship between training scalability and problem size is discussed, in the context of distributed frameworks, also in Section 3. Section 4 concludes the paper. 