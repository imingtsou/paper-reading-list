\section{Introduction}
Deep learning has been quite successful in improving predictive power in domains such as computer vision and natural language processing. State-of-the-art performance in computer vision is driven by the convolutional neural network model, a special kind of feed-forward deep learning model. The high-level idea is to learn images filters for extracting meaningful features and predictions. On the other hand, natural language processing has had a lot of success applying recurrent neural networks, a type of feedback model well-suited for learning order and context-sensitive sequences (as in natural languages). 

As accuracies continue to increase in both domains, so do the complexity of network architectures and the size of the parameter space. Google's network for unsupervised learning of image features reached a billion parameters \cite{donahue2014decaf}, and was increased to 11 billion parmeters in a separate experiment at Stanford \cite{schmidhuber2015deep}. In the NLP space, Digital Reasoning Systems trained a 160 billion parameter network \cite{trask2015modeling} fairly recently. Handling problems of this size involves looking beyond the single machine, which Google first demonstrated through its distributed DistBelief framework \cite{dean2012large}.

The goal of this paper is to survey the landscape of deep learning frameworks with full support for parallelization. Three levels of parallelization exist on the hardware level: within a GPU, between GPUs on a single node, and between nodes. Two forms of parallelism also exist on the application level: model and data parallelism. Other aspects of frameworks include release date, core language, user-facing API, computation model, communication model, deep learning types, programming paradigm, fault tolerance, and visualization. This choice of criteria is explained in detail in Section 2. Tensorflow, CNTK, Deeplearning4j, MXNet, H2O, Caffe, Theano, and Torch do not necessarily encompass the entire space of frameworks for deep learning, but were selected by a combination of factors: their being open-source, level of documentation, maturity as a complete product, and level of adoption by the community. These frameworks, as well as some others not included in the Section 2 chart, are examined in detail in Section 3. Section 4 discusses finer points of parallelism and scalability in deep learning which may escape but are pertinent to the framework discussion. Section 5 offers concluding remarks.