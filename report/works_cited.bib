@article{DBLP:journals/corr/BahrampourRSS15,
  author    = {Soheil Bahrampour and
               Naveen Ramakrishnan and
               Lukas Schott and
               Mohak Shah},
  title     = {Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06435},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06435},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BahrampourRSS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{Large52:online,
author = {},
title = {Large Scale Distributed Deep Learning on Hadoop... | Hadoop at Yahoo},
howpublished = {\url{http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop}},
month = {},
year = {},
note = {(Accessed on 07/23/2016)}
}

@misc{Caffe27:online,
author = {},
title = {CaffeOnSpark Open Sourced for Distributed Deep... | Hadoop at Yahoo},
howpublished = {\url{http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep}},
month = {},
year = {},
note = {(Accessed on 07/23/2016)}
}

@article{DBLP:journals/corr/AbadiABBCCCDDDG16,
author    = {Mart{\'{\i}}n Abadi and
           Ashish Agarwal and
           Paul Barham and
           Eugene Brevdo and
           Zhifeng Chen and
           Craig Citro and
           Gregory S. Corrado and
           Andy Davis and
           Jeffrey Dean and
           Matthieu Devin and
           Sanjay Ghemawat and
           Ian J. Goodfellow and
           Andrew Harp and
           Geoffrey Irving and
           Michael Isard and
           Yangqing Jia and
           Rafal J{\'{o}}zefowicz and
           Lukasz Kaiser and
           Manjunath Kudlur and
           Josh Levenberg and
           Dan Man{\'{e}} and
           Rajat Monga and
           Sherry Moore and
           Derek Gordon Murray and
           Chris Olah and
           Mike Schuster and
           Jonathon Shlens and
           Benoit Steiner and
           Ilya Sutskever and
           Kunal Talwar and
           Paul A. Tucker and
           Vincent Vanhoucke and
           Vijay Vasudevan and
           Fernanda B. Vi{\'{e}}gas and
           Oriol Vinyals and
           Pete Warden and
           Martin Wattenberg and
           Martin Wicke and
           Yuan Yu and
           Xiaoqiang Zheng},
title     = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
           Systems},
journal   = {CoRR},
volume    = {abs/1603.04467},
year      = {2016},
url       = {http://arxiv.org/abs/1603.04467},
timestamp = {Fri, 01 Jul 2016 14:44:15 +0200},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/AbadiABBCCCDDDG16},
bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{Landset2015,
author="Landset, Sara
and Khoshgoftaar, Taghi M.
and Richter, Aaron N.
and Hasanin, Tawfiq",
title="A survey of open source tools for machine learning with big data in the Hadoop ecosystem",
journal="Journal of Big Data",
year="2015",
volume="2",
number="1",
pages="1--36",
abstract="With an ever-increasing amount of options, the task of selecting machine learning tools for big data can be difficult. The available tools have advantages and drawbacks, and many have overlapping uses. The world's data is growing rapidly, and traditional tools for machine learning are becoming insufficient as we move towards distributed and real-time processing. This paper is intended to aid the researcher or professional who understands machine learning but is inexperienced with big data. In order to evaluate tools, one should have a thorough understanding of what to look for. To that end, this paper provides a list of criteria for making selections along with an analysis of the advantages and drawbacks of each. We do this by starting from the beginning, and looking at what exactly the term ``big data'' means. From there, we go on to the Hadoop ecosystem for a look at many of the projects that are part of a typical machine learning architecture and an understanding of how everything might fit together. We discuss the advantages and disadvantages of three different processing paradigms along with a comparison of engines that implement them, including MapReduce, Spark, Flink, Storm, and H2O. We then look at machine learning libraries and frameworks including Mahout, MLlib, SAMOA, and evaluate them based on criteria such as scalability, ease of use, and extensibility. There is no single toolkit that truly embodies a one-size-fits-all solution, so this paper aims to help make decisions smoother by providing as much information as possible and quantifying what the tradeoffs will be. Additionally, throughout this paper, we review recent research in the field using these tools and talk about possible future directions for toolkit-based learning.",
issn="2196-1115",
doi="10.1186/s40537-015-0032-1",
url="http://dx.doi.org/10.1186/s40537-015-0032-1"
}

@misc{candel2015deep,
  title={Deep Learning with H2O},
  author={Candel, Arno and Parmar, Viraj and LeDell, Erin and Arora, Anisha},
  year={2015},
  publisher={H2O}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={693--701},
  year={2011}
}

@inproceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and others},
  booktitle={Advances in neural information processing systems},
  pages={1223--1231},
  year={2012}
}

@inproceedings{trask2015modeling,
  title={Modeling order in neural word embeddings at scale},
  author={Trask, Andrew and Gilmore, David and Russell, Matthew}
}

@inproceedings{donahue2014decaf,
  title={DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.},
  author={Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  booktitle={ICML},
  pages={647--655},
  year={2014}
}

@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@misc{Itera38:online,
author = {},
title = {Iterative Reduce With DL4J on Hadoop and Spark - Deeplearning4j: Open-source, distributed deep learning for the JVM},
howpublished = {\url{http://deeplearning4j.org/iterativereduce}},
month = {},
year = {},
note = {(Accessed on 07/26/2016)}
}

@misc{ND4J'34:online,
author = {},
title = {ND4J's Benchmarking Tool N-Dimensional Scientific Computing for Java},
howpublished = {\url{http://nd4j.org/benchmarking}},
month = {},
year = {},
note = {(Accessed on 07/26/2016)}
}

@article{chen2015mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}