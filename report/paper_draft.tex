\documentclass{article}

% Preamble
\usepackage{url}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont

\newcommand{\xmark}{\ding{55}}%

% Document
\begin{document}
\section{Abstract}
The study and adoption of deep learning methods has led to significant progress in different application domains. As deep learning continues to show promise and its utilization matures, so does the infrastructure and software needed to support it. Various frameworks have been developed in recent years to facilitate both implementation and training of deep learning networks. As deep learning has also evolved to becoming distributed, there's a growing need for frameworks that can support execution beyond a single machine. While deep learning frameworks restricted to running on a single machine have been studied and compared, frameworks which support deep learning distributed across multiple machines are relatively less known and well-studied. This paper seeks to bridge that gap by surveying, summarizing, and comparing frameworks which currently support distributed execution, including Tensorflow, MXNet, Deeplearning4j, and H2O. 

\section{Introduction}
\begin{itemize}
\item Application domain examples: vision, speech recognition, NLP
\item Architecture types:  CNNs, RNNs, fully-connected, auto-encoders, etc.
\item Overview of what deep learning involves? E.g. training data, objective function, optimization, tricks for avoiding overfitting; network-agnostic issues. 
\item Big data and progress of deep learning implementations (CPU to GPU to distributed)
\item Motivation for distributed deep learning (ref. Jeff Dean paper; seminal?)
\item Introduce paper motivation; distributed not well-studied
\item Mention all the frameworks chosen (including non-distributed); emphasis on distributed.
\item "The rest of the paper is organized as follows ..."
\end{itemize}

\section{Overview of Deep Learning Frameworks}
Applications: LeNet, AlexNet, Stacked autoencoders, LSTM.

Results:

Torch is fastest even on every condition. TensorFlow does well on CPU (except on Stacked autoencoders) but does worst on GPU. Theano does pretty good with GPU, even on some special applications with CPU. On LSTM, Theano is better on GPU while Torch is better on CPU.

Comments:

This comparison is based on single node, including several mainly used frameworks (Torch, TensorFlow, Theano, Caffe, etc). According to the results of the four applications, Torch does the best while Theano follows. TensorFlow, which attracts most attention, doesn't perform well on GPU.

If we want to get further results, we should compare them on distributed clusters and use the latest version. This paper is submitted on March 2016, and during the four months, I do believe the frameworks especially TensorFlow evolves a lot.
\section{Comparison Table}

\begin{tabularx}{1.3\textwidth}{ |X|X|X|X|X|X| }
  \hline
  Platform & Tensorflow & Deeplearning4j & MXNet & H2O & CaffeOnSpark \\
  \hline
  Release Date & & & & & \\
   \hline
  Popularity & & & & & \\
  \hline 
  Core & item 2  & item 3  & item 4 & label 5 & label 6 \\
  \hline
  API & & & & & \\
  \hline
  Computation Model & synchronous or asynchronous & synchronous & synchronous or asynchronous & & synchronous \\
  \hline
  Communication Model & & & & & \\
  \hline
  Data Parallelism & \checkmark & \checkmark & \checkmark& \checkmark & \checkmark \\
  \hline
  Model Parallelism & \checkmark & \xmark & \checkmark & & \xmark \\
  \hline
  Programming Paradigm & Imperative & Declarative & Both & & Declarative \\
  \hline
  Fault Tolerance & & & & & \\
  \hline
  UI/Visualization & & & & & \\
  \hline
\end{tabularx}

\section{Framework Discussion}
\subsection{MXNet}
MXNet is a distributed deep learning framework that became available in 2015. It was developed with collaborators from several institutions, including CMU, University of Washington, and Microsoft. It currently interfaces with C++, Python, R, Scala, Matlab, Javascript, Go, and Julia. MXNet supports both declarative and declarative expressions; symbolic in declaring computation graphs with higher-level abstractions like convolutional layers, and imperative in the ability to direct tensor computation and control flow [1]. Data parallelism is supported by default, and it also seems possible to build with model parallelism. Distributed execution in MXNet generally follows a parameter server model, with parallelism and data consistency managed at two levels: intra-worker and inter-worker. Devices within a single worker machine maintain synchronous consistency on its parameters. Inter-worker data consistency can either be synchronous, where gradients over all workers are aggregated before proceeding, or asynchronous, where each worker independently updates parameters. This trade-off between performance and convergence speed is left as an option to the user. The actual handling of server updates and requests is pushed down to MXNet's dependency engine, which schedules all operations and performs resource management. Results from MXNet's own scaling benchmarking, using Googlenet, show good scaling from 1 to 10 machines [1]. Fault tolerance on MXNet involves checkpoint-and-restart, which must be user-initiated. 

\subsection{Deeplearning4j}
Deeplearning4j is a Java-based deep learning library built and supported by Skymind, a machine learning intelligence company founded in 2014. It is an open source product designed for adoptability in industry, where Java is very common. The framework currently interfaces with both Java and Scala, with a Python SDK in-progress. Programming is primarily declarative, involving specifying network hyperparameters and layer information. Deeplearning4j integrates with Hadoop and Spark, or Akka and AWS for processing backends. Distributed execution provides data parallelism through the Iterative MapReduce model (\textbf{ref?}). Each worker processes its own minibatch of training data, with workers periodically "reducing" (averaging) their parameter data. Formal benchmarking in terms of scaling was not found, but benchmarking on their custom Java linear algebra library show 2x or more speedup over Numpy on large matrix multiplies. Deeplearing4j's website provides clear documentation of available features and API, which range from range from a menu of optimization algorithms to built-in vectorization libraries. Fault tolerance is not mentioned, although Spark does have built-in fault tolerance mechanisms. 

\subsection{CaffeOnSpark}
CaffeOnSpark is a Spark deep learning package released as open-source in early 2016 by Yahoo's Big ML team. It serves as a distributed implementation of Caffe, a framework for convolutional deep learning released by UC Berkeley's computer vision community in 2014. The language interface for CaffeOnSpark is Scala (following Spark), while Caffe itself offers Python and Matlab API. Programming is declarative; creating a deep learning network involves specifying layers and hyperparameters, which are compiled down to a configuration file that Caffe then uses. During distributed runtime, Spark launches "executors," each responsible for a partition of HDFS-based training data and trains the data by running multiple Caffe threads mapped to GPUs \cite{Large52:online}. MPI is used to synchronize executor's respective the parameters' gradients in an Allreduce-like fashion, per training batch \cite{Caffe27:online}. In terms of some notable features, Caffe itself hosts a repository of pre-trained models of some popular convolutional networks such as AlexNet or GoogleNet. It also integrates support for data preprocessing, including building LMDB databases from raw data for higher-throughput, concurrent reading. It does not seem from surveyed literature that CaffeOnSpark offers any fault tolerance other than what comes with Spark.

\subsection{Tensorflow}
Tensorflow was released by Google Research for open source in November 2015, and updated to include distributed support in 2016. The user-facing APIs are C++ and Python. Programming with Tensorflow leans more imperative than with some of the other frameworks discussed. While plenty of abstraction power is expressed in its library, the user will probably also be working with computational wrappers of primitives such as matrix operations, element-wise math operators, and looping control. In other words, the user is exposed to some of the internal workings of deep learning networks. Tensorflow sees constructed networks as a directed graph of nodes encapsulating dataflow computation and required dependencies \cite{DBLP:journals/corr/AbadiABBCCCDDDG16}. Each node, or computation, gets mapped to devices (CPUs or GPUs) according to some cost function. This partitions the overall graph into subgraphs, one per device. Cross-device edges are replaced to encode necessary synchronization between device pairs. Distributed execution appears to be a natural extension of this arrangement, except that TCP or Remote Direct Memory Access (RDMA) is used for inter-device communication on separate machines. This approach of mapping subgraphs onto devices also offers scalability potential, as each worker can schedule its own subgraph at runtime instead of relying on a centralized master \cite{DBLP:journals/corr/AbadiABBCCCDDDG16}. Parallelism in Tensorflow can be expressed at several levels, notable both data parallelism and model parallelism. Data parallelism can happen both across and within workers, by training separate batches of data on model replications. Model parallelism is expressed through splitting one model, or its graph, across devices. Model updates can either be synchronous or asynchronous for parameter-optimizing algorithms such as stochastic gradient descent (SGD). For fault tolerance, Tensorflow provides checkpointing and recovery of data designated to be persistent, while the overall computation graph is restarted. In terms of other features, TensorBoard is a tool for interactive visualization of a user's network, and also provides time series data on various aspects of the learning network's state during training. 

\subsection{H2O}
H2O is the open-source product of H2O.ai, a company focused on machine learning solutions. H2O is unique among the other tools discussed here in that it is a complete data processing platform, with its own parallel processing engine (improving on MapReduce) with a general machine learning library and other tools. The discussion will be limited to H2O's deep learning component, available since 2014. H2O is Java-based at its core, but also offers API support for Java, R, Python, Scala, Javascript, as well as a web-UI interface \cite{candel2015deep}. Programming for deep learning appears declarative, as model-building involves specifying hyperparameters and high-level layer information. Distributed execution for deep learning follows the characteristics of H2O's processing engine, which is in-memory and can be summarized as a distributed fork-join model (targeting finer-grained parallelism) \cite{Landset2015}. Data parallelism is enabled following the "HogWild!" \cite{recht2011hogwild} approach for parallelizing SGD. In implementation, multiple cores handle subsets of training data and update shared parameters asynchronously. Scaling up to multi-node, each node operates in parallel on a copy of the global parameters, while parameters are averaged for a global update, per training iteration \cite{candel2015deep}. There does not seem to be explicit support for model parallelism. Fault tolerance involves user-initiated checkpoint-and-resume. H2O's web tool can be used to build models and manage workflows, and also provides some basic summary statistics, e.g. confusion matrix from training and validation. 

\section{Future Work}
\section{Conclusion}

\bibliography{works_cited} 
\bibliographystyle{ieeetr}

\end{document}

